
# Setting up Ceph from source
This guide shows the installation and configuration of Ceph. In the following, we use three backend hosts (OSDs), one monitor host, and one gateway host (Rados Gateway). There should be one extra host acting as a client. We assume all hosts have the same environment, configuration, and software installed. 
## Download Ceph
Use Ubuntu Jammy 22.04 LTS. Install dependencies. Ensure Python is not installed.
### Setup basic dependencies
Install build toolchains.
```bash
apt update
apt install vim build-essential git python3.10
```
### Clone Ceph and install dependencies
Clone the ceph source code, use the latest v17.2.6 (Quincy), apply `ceph_xo.patch` from this repo, and install dependencies. Ensure to create the `man1` folder to avoid failure in setting up OpenJDK.
```bash
git clone https://github.com/ceph/ceph
cd ceph
git checkout v17.2.6
git apply ceph_xo.patch
mkdir -p /usr/share/man/man1
./install-deps.sh
```
### Build and install Ceph
Ensure the following is in the `do_cmake.sh` script: `ARGS+=" -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_MGR_DASHBOARD_FRONTEND=OFF -DWITH_MANPAGE=OFF"`. Run the script, and build Ceph. Install Ceph, and ignore the error regarding missing `BUILD_DOC` when Cython is running.
```bash
./do_cmake.sh
cd build
ninja -v
ninja install
```
### Configure hostnames
Ceph may use hostnames to connect between components in different servers. Configure them in `/etc/hosts`. For example:
```
192.168.11.153	n04	# ceph mon
192.168.11.133	n05	# ceph rgw
192.168.11.142	n09	# ceph OSD0
192.168.11.161	n11	# ceph OSD1
192.168.11.144	n12	# ceph OSD2
```
## Configure monitor
Create a user called `ceph`. Create required folders.
### Create essential folders
```bash
useradd ceph
mkdir -p /etc/ceph
mkdir -p /var/run/ceph
mkdir -p /var/lib/ceph
chown ceph:ceph /etc/ceph
chown ceph:ceph /var/run/ceph
chown ceph:ceph /var/lib/ceph
```
### Create the configuration file
Create `/etc/ceph/ceph.conf` with the following. Replace the IP addresses (e.g., `192.168.11.153`, `192.168.11.0/24`) and hostnames (`n04`) as appropriate.
```
[global]
fsid = 03810e63-857e-4410-80ce-1232adc8d711
mon initial members = n04
mon host = 192.168.11.153
public network = 192.168.11.0/24
cluster network = 192.168.11.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd_pool_default_size = 3
osd_pool_default_min_size = 2
osd_pool_default_pg_num = 333
osd_crush_chooseleaf_type = 1

[mon.n04]
host = n04
mon allow pool delete = true
#mon data avail crit = 3

[mgr.n04]
host = n04

[osd.0]
host = n09

[osd.1]
host = n11

[osd.2]
host = n12
```
Replace `fsid` with an ID generated by running `uuid`.
### Create authentication for the monitor
Run the following on the monitor host (`n04`). Without doing this would cause issues later on with the OSD and Rados Gateway.
```bash
# ceph config set global osd_class_dir /usr/local/lib/rados-classes
# export export PYTHONPATH=/usr/local/lib/python3.10/dist-packages:/usr/local/lib/python3/dist-packages
```
Create the first keyring.
```bash
ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'
mkdir -p /var/lib/ceph/bootstrap-osd
ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'
ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
chown ceph:ceph /tmp/ceph.mon.keyring
monmaptool --create --add n04 192.168.11.153 --fsid 03810e63-857e-4410-80ce-1232adc8d711 /tmp/monmap
mkdir /var/lib/ceph/mon/ceph-n04
sudo -u ceph ceph-mon --mkfs -i n04 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring
systemctl start ceph-mon@n04
```
At this point, check if you can run Ceph and at least the monitor is correct. Make sure the Python path is configured.
```bash
# ceph -s
  cluster:
    id:     03810e63-857e-4410-80ce-1232adc8d711
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum n04 (age 4s)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     
```
## Setup the OSDs
SSH into the first OSD host. Use `fdisk` to delete all partitions in the disk drive (e.g. `/dev/nvme0n1`, add a `DOS` label, save and quit. Run the `ceph-volume` tool.
```bash
ceph-volume lvm create --data /dev/nvme0n1
```
Repeat this in all the OSD hosts. In the end, `ceph -s` should return how many OSDs are up:
```bash
...
    osd: 3 osds: 3 up (since 81m), 3 in (since 22h)
...
```
## Setup Ceph manager
Create the key for the manager.
```bash
mkdir -p /var/lib/ceph/mgr/ceph-n04
ceph auth get-or-create mgr.$name mon 'allow profile mgr' osd 'allow *' mds 'allow *'
```
Put the output on the screen in `/var/lib/ceph/mgr/ceph-n04/keyring`.
Start the manager.
```bash
# pip3 install jsonpatch
# systemctl start ceph-mgr@n04
# systemctl status ceph-mgr@n04
● ceph-mgr@n04.service - Ceph cluster manager daemon
     Loaded: loaded (/lib/systemd/system/ceph-mgr@.service; disabled; vendor preset: enabled)
     Active: active (running) since Wed 2023-07-26 12:52:21 UTC; 23min ago
   Main PID: 105627 (ceph-mgr)
      Tasks: 21 (limit: 18894)
     Memory: 336.1M
        CPU: 2min 9.103s
     CGroup: /system.slice/system-ceph\x2dmgr.slice/ceph-mgr@n04.service
             └─105627 /usr/local/bin/ceph-mgr -f --cluster ceph --id n04 --setuser ceph --setgroup ceph

Jul 26 13:14:51 n04 ceph-mgr[105627]: 2023-07-26T13:14:51.841+0000 7f76448b7000 -1 mgr[py] Module devicehealth has missing NOTIFY_TYPES member
Jul 26 13:14:51 n04 ceph-mgr[105627]: 2023-07-26T13:14:51.953+0000 7f76448b7000 -1 mgr[py] Module alerts has missing NOTIFY_TYPES member
Jul 26 13:14:52 n04 ceph-mgr[105627]: 2023-07-26T13:14:52.881+0000 7f76448b7000 -1 mgr[py] Module progress has missing NOTIFY_TYPES member
Jul 26 13:14:53 n04 ceph-mgr[105627]: 2023-07-26T13:14:53.281+0000 7f76448b7000 -1 mgr[py] Module prometheus has missing NOTIFY_TYPES member
Jul 26 13:14:53 n04 ceph-mgr[105627]: 2023-07-26T13:14:53.381+0000 7f76448b7000 -1 mgr[py] Module influx has missing NOTIFY_TYPES member
Jul 26 13:14:53 n04 ceph-mgr[105627]: 2023-07-26T13:14:53.697+0000 7f76448b7000 -1 mgr[py] Module volumes has missing NOTIFY_TYPES member
Jul 26 13:14:54 n04 ceph-mgr[105627]: 2023-07-26T13:14:54.957+0000 7f76448b7000 -1 mgr[py] Module status has missing NOTIFY_TYPES member
Jul 26 13:14:55 n04 ceph-mgr[105627]: 2023-07-26T13:14:55.209+0000 7f76448b7000 -1 mgr[py] Module pg_autoscaler has missing NOTIFY_TYPES member
Jul 26 13:14:55 n04 ceph-mgr[105627]: 2023-07-26T13:14:55.717+0000 7f76448b7000 -1 mgr[py] Module rbd_support has missing NOTIFY_TYPES member
Jul 26 13:14:55 n04 ceph-mgr[105627]: 2023-07-26T13:14:55.805+0000 7f76448b7000 -1 mgr[py] Module iostat has missing NOTIFY_TYPES member
```
Ignore the error regarding `NOTIFY_TYPES`. However, ensure there are no other Python-related errors by checking `journalctl`.
Check `ceph -s` should return:
```bash
...
mgr: n04(active, starting, since 2s)
...
```
## Finishing touches
Enable messenger v2 protocol.
```bash
ceph mon enable-msgr2
```
Disable insecure global ID warning.
```bash
ceph config set mon mon_warn_on_insecure_global_id_reclaim false  
ceph config set mon mon_warn_on_insecure_global_id_reclaim_allowed false  

ceph config set mon mon_warn_on_insecure_global_id_reclaim true  
ceph config set mon mon_warn_on_insecure_global_id_reclaim_allowed true
ceph config set mon auth_allow_insecure_global_id_reclaim false
```
## Test librados
Test if librados work. With the following.
```python3
import rados

# connect to monitor
cluster = rados.Rados(conffile='/etc/ceph/ceph.conf')
cluster.connect()

# Create a pool
cluster.create_pool('test')
print(cluster.list_pools())

# Open an I/O context to the pool
ioctx = cluster.open_ioctx('test')

# Put object
ioctx.write_full("hw", b'hello world')

# Get object
print(ioctx.read('hw'))

#delete pool
cluster.delete_pool('test')
print(cluster.list_pools())

# Shutdown connect to the cluster
cluster.shutdown()
```
## Configure Rados Gateway
Configure Rados Gateway to enable S3 access from web clients. SSH into the machine where the gateway is installed.
```bash
mkdir -p /var/lib/ceph/radosgw/ceph-rgw.`hostname -s`
```
Create entity and keyring.
```bash
ceph auth get-or-create client.rgw.`hostname -s` osd 'allow rwx' mon 'allow rw' -o /var/lib/ceph/radosgw/ceph-rgw.`hostname -s`/keyring
chown -R ceph:ceph /var/lib/ceph/radosgw
chown -R ceph:ceph /var/log/ceph
chown -R ceph:ceph /var/run/ceph
chown -R ceph:ceph /etc/ceph
```
Self sign a SSL certificate to allow HTTPS.
``` bash
openssl req -newkey rsa:2048 -nodes -keyout /var/lib/ceph/radosgw/ceph-rgw.`hostname -s`/`hostname -s`-key.pem -x509 -days 365 -out /var/lib/ceph/radosgw/ceph-rgw.`hostname -s`/`hostname -s`-crt.pem
```
Add the following configuration to `/etc/ceph/ceph.conf`. Change `n05` and the IP addresses to the correct ones. Ensure that there is now space between the key and value between the `=` in the `rgw_frontends` configuration.
```
[client.rgw.n05]
host = n05
keyring = /var/lib/ceph/radosgw/ceph-rgw.n05/keyring
rgw_frontends = beast endpoint=192.168.11.133:80 verify_ssl=false ssl_endpoint=192.168.11.133:443 ssl_certificate=/var/lib/ceph/radosgw/ceph-rgw.n05/n05-crt.pem ssl_private_key=/var/lib/ceph/radosgw/ceph-rgw.n05/n05-key.pem
```
Start the gateway and check its status.
```bash
# systemctl restart ceph-radosgw@rgw.`hostname -s`
# systemctl status ceph-radosgw@rgw.`hostname -s`
● ceph-radosgw@rgw.n05.service - Ceph rados gateway
     Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: enabled)
     Active: active (running) since Mon 2023-07-31 11:19:52 UTC; 3s ago
   Main PID: 1905 (radosgw)
      Tasks: 603
     Memory: 79.8M
        CPU: 363ms
     CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.n05.service
             └─1905 /usr/local/bin/radosgw -f --cluster ceph --name client.rgw.n05 --setuser ceph --setgroup ceph

Jul 31 11:19:52 n05 systemd[1]: Started Ceph rados gateway.

```
Ensure that the gateway is responsive by sending a GET request from HTTP and HTTPS.
```bash
root@n06:~# curl http://n05
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>root@n06:~# 
root@n06:~# curl -k https://n05
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>root@n06:~# 
```
### Create gateway admin user
Create the initial user in the gateway and assign admin capability. This is the gateway user, not a RADOS user.
```bash
radosgw-admin user create --uid=admin --display-name="admin"
radosgw-admin caps add --uid=admin --caps="users=*"
```
Save the following from the on-screen output:
```
"keys": [ { "user": "admin-api-user", "access_key": "...", "secret_key": "..." }
```
Put it in a S3 credential file.
```bash
mkdir ~/.aws
vim ~/.aws/credentials
```
```
[default]
aws_access_key_id = ...
aws_secret_access_key = ...
```
### Test the gateway
Install the Boto3 package.
```bash
pip3 install boto3
```
Run the following in Python3.
```python3
import boto3

s3 = boto3.resource('s3', use_ssl=True, verify=False, endpoint_url='https://n05', aws_access_key_id='...', aws_secret_access_key='...')

# create bucket
s3.create_bucket(Bucket='test')
bucket = s3.Bucket('test')

# Create an object
bucket.put_object(Key='hello', Body='world')

# List buckets
for bucket in s3.buckets.all():
    print(bucket)

# list objects
for object in bucket.objects.all():
    print(object)

# Get object
obj = s3.Object(bucket_name='test', key='hello')
print(obj.get()['Body'].read())

# Delete object
print(obj.delete())

# Delete bucket
print(bucket.delete())
```
Running the script should return the following.
```bash
# python3 test.py 
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
s3.Bucket(name='test')
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
s3.ObjectSummary(bucket_name='test', key='hello')
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
b'world'
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
{'ResponseMetadata': {'RequestId': 'tx000002aa7860f8e075e07-0064c797fc-1b67e-default', 'HostId': '', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-request-id': 'tx000002aa7860f8e075e07-0064c797fc-1b67e-default', 'date': 'Mon, 31 Jul 2023 11:16:12 GMT', 'connection': 'Keep-Alive'}, 'RetryAttempts': 0}}
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1015: InsecureRequestWarning: Unverified HTTPS request is being made to host '192.168.11.133'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  warnings.warn(
{'ResponseMetadata': {'RequestId': 'tx00000ef4a30582232c905-0064c797fc-1b67e-default', 'HostId': '', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-request-id': 'tx00000ef4a30582232c905-0064c797fc-1b67e-default', 'date': 'Mon, 31 Jul 2023 11:16:12 GMT', 'connection': 'Keep-Alive'}, 'RetryAttempts': 0}}
```
## Reboot
After rebooting the cluster, or part of the cluster. Restart the services in order.
### Monitor and Manager
```bash
systemctl start ceph-mon@`hostname -s`
systemctl start ceph-mgr@`hostname -s`
```
### OSD
Then restart the OSDs one by one. First, check the OSD ID and the FSID using `ceph-volume`.
```bash
# ceph-volume lvm list


====== osd.1 =======

  [block]       /dev/ceph-7f302848-026c-42dc-9e4f-faa63ce07e63/osd-block-8ee65f8b-53ef-433b-b9b5-fdc5e301a73a

      block device              /dev/ceph-7f302848-026c-42dc-9e4f-faa63ce07e63/osd-block-8ee65f8b-53ef-433b-b9b5-fdc5e301a73a
      block uuid                qRZr62-IsBJ-sgBL-2CDq-nxIL-21OX-WR02CD
      cephx lockbox secret      
      cluster fsid              03810e63-857e-4410-80ce-1232adc8d711
      cluster name              ceph
      crush device class        
      encrypted                 0
      osd fsid                  8ee65f8b-53ef-433b-b9b5-fdc5e301a73a
      osd id                    1
      osdspec affinity          
      type                      block
      vdo                       0
      devices                   /dev/nvme0n1
```
Restart the OSD using the activate command.
```bash
# ceph-volume lvm activate 1 8ee65f8b-53ef-433b-b9b5-fdc5e301a73a
Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1
--> Executable selinuxenabled not in PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1
Running command: /usr/local/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-7f302848-026c-42dc-9e4f-faa63ce07e63/osd-block-8ee65f8b-53ef-433b-b9b5-fdc5e301a73a --path /var/lib/ceph/osd/ceph-1 --no-mon-config
Running command: /usr/bin/ln -snf /dev/ceph-7f302848-026c-42dc-9e4f-faa63ce07e63/osd-block-8ee65f8b-53ef-433b-b9b5-fdc5e301a73a /var/lib/ceph/osd/ceph-1/block
Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block
Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-1
Running command: /usr/bin/systemctl enable ceph-volume@lvm-1-8ee65f8b-53ef-433b-b9b5-fdc5e301a73a
Running command: /usr/bin/systemctl enable --runtime ceph-osd@1
 stderr: Created symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@1.service → /lib/systemd/system/ceph-osd@.service.
Running command: /usr/bin/systemctl start ceph-osd@1
--> ceph-volume lvm activate successful for osd ID: 1
```
Restart all the OSDs one by one this way.
### Rados Gateway
Restart the gateway service.
```bash
systemctl start ceph-radosgw@rgw.`hostname -s`
```
### Check health
Finally, check the cluster status to ensure the system is all up and running. Ensure that all the placement groups and active and clean.
```bash
  cluster:
    id:     03810e63-857e-4410-80ce-1232adc8d711
    health: HEALTH_WARN
            mon n04 is low on available space
 
  services:
    mon: 1 daemons, quorum n04 (age 12m)
    mgr: n04(active, since 28s)
    osd: 3 osds: 3 up (since 83s), 3 in (since 3d)
    rgw: 1 daemon active (1 hosts, 1 zones)
 
  data:
    pools:   7 pools, 193 pgs
    objects: 250 objects, 9.6 MiB
    usage:   448 MiB used, 4.4 TiB / 4.4 TiB avail
    pgs:     193 active+clean
 
```
